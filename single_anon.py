import os
from diffusers.utils import load_image
import torch
from transformers import CLIPImageProcessor, CLIPVisionModel
from diffusers import AutoencoderKL, DDPMScheduler
from diffusers.utils import load_image
from src.diffusers.models.referencenet.referencenet_unet_2d_condition import (
    ReferenceNetModel,
)
from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel
from src.diffusers.pipelines.referencenet.pipeline_referencenet import (
    StableDiffusionReferenceNetPipeline,
)
import face_alignment
from utils.anonymize_faces_in_image import anonymize_faces_in_image
import sys

# ğŸ“Œ 2ï¸âƒ£ ìµëª…í™” ëª¨ë¸ ë¡œë“œ
face_model_id = "hkung/face-anon-simple"
clip_model_id = "openai/clip-vit-large-patch14"
sd_model_id = "stabilityai/stable-diffusion-2-1"

unet = UNet2DConditionModel.from_pretrained(
    face_model_id, subfolder="unet", use_safetensors=True
)
referencenet = ReferenceNetModel.from_pretrained(
    face_model_id, subfolder="referencenet", use_safetensors=True
)
conditioning_referencenet = ReferenceNetModel.from_pretrained(
    face_model_id, subfolder="conditioning_referencenet", use_safetensors=True
)
vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder="vae", use_safetensors=True)
scheduler = DDPMScheduler.from_pretrained(
    sd_model_id, subfolder="scheduler", use_safetensors=True
)
feature_extractor = CLIPImageProcessor.from_pretrained(
    clip_model_id, use_safetensors=True
)
image_encoder = CLIPVisionModel.from_pretrained(clip_model_id, use_safetensors=True)

pipe = StableDiffusionReferenceNetPipeline(
    unet=unet,
    referencenet=referencenet,
    conditioning_referencenet=conditioning_referencenet,
    vae=vae,
    feature_extractor=feature_extractor,
    image_encoder=image_encoder,
    scheduler=scheduler,
)


pipe = pipe.to("cuda")

generator = torch.manual_seed(1)  # ëœë¤ì„± ê³ ì •í•˜ì—¬ ë™ì¼í•œ ê²°ê³¼ ì¶œë ¥


def get_single_anon(input_folder, output_folder, num_inference_steps, anonymization_degree):
    # ìµëª…í™”í•  ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
    # input_folder = "my_dataset/original/"
    # output_folder = "my_dataset/anonymized/"

    # ì…ë ¥ í´ë”ê°€ ì—†ìœ¼ë©´ ê²½ê³  í›„ ì¢…ë£Œ
    if not os.path.exists(input_folder):
        print(f"ê²½ê³ : ]]ì…ë ¥ í´ë” '{input_folder}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", file=sys.stderr)
        sys.exit(1)

    # ì¶œë ¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(output_folder, exist_ok=True)

    # í´ë” ë‚´ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    image_files = [f for f in os.listdir(input_folder) if f.endswith((".png", ".jpg", ".jpeg"))]

    # ì´ë¯¸ì§€ ìµëª…í™” ì§„í–‰
    for image_file in image_files:
        input_path = os.path.join(input_folder, image_file)  # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ
        # íŒŒì¼ëª…ê³¼ í™•ì¥ì ë¶„ë¦¬
        filename, ext = os.path.splitext(image_file)
        
        # ìµëª…í™”ëœ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ (ì›ë³¸íŒŒì¼ëª…_anon.í™•ì¥ì)
        output_path = os.path.join(output_folder, f"{filename}{ext}")
        
        print(f"ğŸ”„ ìµëª…í™” ì§„í–‰: {image_file}")

        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
        original_image = load_image(input_path)
        # SFD (likely best results, but slower)

        fa = face_alignment.FaceAlignment(
        face_alignment.LandmarksType.TWO_D, face_detector="sfd"
        )
        # generate an image that anonymizes faces
        anon_image = anonymize_faces_in_image(
            image=original_image,
            face_alignment=fa,
            pipe=pipe,
            generator=generator,
            face_image_size=512,
            num_inference_steps=num_inference_steps,
            guidance_scale=4.0,
            anonymization_degree=anonymization_degree,
        )
        # ì´ë¯¸ì§€ ì €ì¥
        anon_image.save(output_path)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")


    print("ğŸ‰ ëª¨ë“  ì´ë¯¸ì§€ ìµëª…í™” ì™„ë£Œ!")
