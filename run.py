import torch
from transformers import CLIPImageProcessor, CLIPVisionModel
from diffusers import AutoencoderKL, DDPMScheduler
from diffusers.utils import load_image, make_image_grid
from src.diffusers.models.referencenet.referencenet_unet_2d_condition import (
    ReferenceNetModel,
)
from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel
from src.diffusers.pipelines.referencenet.pipeline_referencenet import (
    StableDiffusionReferenceNetPipeline,
)
import face_alignment
from utils.anonymize_faces_in_image import anonymize_faces_in_image
from face_alignment import FaceAlignment, LandmarksType
from utils.face_embedding import save_exclusion_faces
import os

# ğŸ“Œ 2ï¸âƒ£ ìµëª…í™” ëª¨ë¸ ë¡œë“œ
face_model_id = "hkung/face-anon-simple"
clip_model_id = "openai/clip-vit-large-patch14"
sd_model_id = "stabilityai/stable-diffusion-2-1"

unet = UNet2DConditionModel.from_pretrained(
    face_model_id, subfolder="unet", use_safetensors=True
)
referencenet = ReferenceNetModel.from_pretrained(
    face_model_id, subfolder="referencenet", use_safetensors=True
)
conditioning_referencenet = ReferenceNetModel.from_pretrained(
    face_model_id, subfolder="conditioning_referencenet", use_safetensors=True
)
vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder="vae", use_safetensors=True)
scheduler = DDPMScheduler.from_pretrained(
    sd_model_id, subfolder="scheduler", use_safetensors=True
)
feature_extractor = CLIPImageProcessor.from_pretrained(
    clip_model_id, use_safetensors=True
)
image_encoder = CLIPVisionModel.from_pretrained(clip_model_id, use_safetensors=True)

pipe = StableDiffusionReferenceNetPipeline(
    unet=unet,
    referencenet=referencenet,
    conditioning_referencenet=conditioning_referencenet,
    vae=vae,
    feature_extractor=feature_extractor,
    image_encoder=image_encoder,
    scheduler=scheduler,
)

# ğŸ“Œ 3ï¸âƒ£ ë””ë°”ì´ìŠ¤ ì„¤ì • (Mac ì‚¬ìš©ìëŠ” MPS í™œìš©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pipe = pipe.to(device)

generator = torch.manual_seed(1)  # ëœë¤ì„± ê³ ì •í•˜ì—¬ ë™ì¼í•œ ê²°ê³¼ ì¶œë ¥
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# ğŸ“Œ 4ï¸âƒ£ ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ (íŠ¹ì • ì¸ë¬¼ ì œì™¸ ìµëª…í™” ëŒ€ìƒ)
original_image = load_image("my_dataset/original/3.jpg")

# ğŸ“Œ 5ï¸âƒ£ ì–¼êµ´ ëœë“œë§ˆí¬ íƒì§€ê¸° ì´ˆê¸°í™”
fa = FaceAlignment(LandmarksType.TWO_D, device=device)

# ğŸ“Œ 6ï¸âƒ£ íŠ¹ì • ì¸ë¬¼ ì œì™¸í•˜ê³  ìµëª…í™” ìˆ˜í–‰
anon_image = anonymize_faces_in_image(
    image=original_image,
    face_alignment=fa,
    pipe=pipe,
    generator=generator,
    face_image_size=512,
    num_inference_steps=25,
    guidance_scale=4.0,
    anonymization_degree=1.25,  # ìµëª…í™” ê°•ë„ ì¡°ì ˆ ê°€ëŠ¥
)
output_path= ("my_dataset/anonymized/3_anon.jpg")
anon_image.save(output_path)
# ğŸ“Œ 7ï¸âƒ£ ìµëª…í™” ì „í›„ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±
# grid_image=make_image_grid([original_image, anon_image], rows=1, cols=2)
# grid_image.show()